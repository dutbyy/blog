## 梯度下降法

维基百科: 

​		**梯度下降法**（英语：Gradient descent）是一个一阶[最优化](https://zh.wikipedia.org/wiki/最优化)[算法](https://zh.wikipedia.org/wiki/算法)，通常也称为**最陡下降法**，但是不该与近似积分的最陡下降法（英语：Method of steepest descent）混淆。 要使用梯度下降法找到一个函数的[局部极小值](https://zh.wikipedia.org/wiki/最值)，必须向函数上当前点对应[梯度](https://zh.wikipedia.org/wiki/梯度)（或者是近似梯度）的*反方向*的规定步长距离点进行[迭代](https://zh.wikipedia.org/wiki/迭代)搜索。如果相反地向梯度*正方向*迭代进行搜索，则会接近函数的[局部极大值](https://zh.wikipedia.org/wiki/最值)点；这个过程则被称为**梯度上升法**。

​		**梯度**(Grdient) 是对多元导数的概括。一元函数就是导数，多元函数是以偏导为分量的向量。		

​		沿着梯度相反的**方向**，函数的值下降最快。下降指的是函数值的下降。

```markdown
	以一元函数为例，x‘处导数>0时，在x'的近邻区域函数值都是在增加的；那么想要使函数的值减小，则需要让x'减少，即沿着导数(>0)相反的方向。
	同理, 多元函数的梯度向量(a,b,c), 每个偏导的反向都能使得函数值减小, 和梯度向量相反的方向, 函数下降的最快(快!!!, 其实只要和梯度的夹角cos(,)小于0就是下降的)。
```

​	求解模型的系数时，损失函数的最小化是一个最优化的任务。所以使用梯度下降时，将要求解的系数A作为自变量，损失函数作为函数。	

​	使用梯度下降求解损失函数的最小值，所以梯度下降的关键是求解损失函数F的导数。

​																						

​	

